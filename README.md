ğŸš¢ TITANIC_ETL - Data Pipeline Project

ğŸ“Œ Overview

TITANIC_ETL is a data pipeline project that processes the Titanic dataset using Apache Airflow. It follows ETL (Extract, Transform, Load) principles to automate data ingestion, cleaning, transformation, and storage.

ğŸ— Project Structure

TITANIC_ETL/
â”œâ”€â”€ dags/                  # Airflow DAGs for workflow automation
â”œâ”€â”€ pipeline/              # ETL scripts for data processing
â”œâ”€â”€ src/                   # Core source code
â”œâ”€â”€ notebook/              # Jupyter notebooks for analysis
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ logs/                  # Logs for debugging
â”œâ”€â”€ tests/                 # Unit tests for validation
â”œâ”€â”€ Dockerfile             # Containerization setup
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation

ğŸš€ Features

âœ… Automated ETL Pipeline using Apache Airflowâœ… Scalable & Configurable architectureâœ… Docker Support for easy deployment
